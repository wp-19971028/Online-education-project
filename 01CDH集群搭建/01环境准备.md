# 集群资源规划

## 业务集群规划思路

> 一般而言，一个集群上很少只跑一个业务，大多数情况都是多个业务共享集群，实际上就是共享系统软硬件资源。

> 这里通常涉及两大问题，其一是业务之间资源隔离问题，就是将各个业务在逻辑上隔离开来，互相不受影响，这个问题产生于业务共享场景下一旦某一业务一段时间内流量猛增必然会因为过度消耗系统资源而影响其他业务；其二就是共享情况下如何使得系统资源利用率最高，理想情况下当然希望集群中所有软硬件资源都得到最大程度利用。

> 因为业务隔离场景是不尽相同的，这里主要针对后者进行讲解：使得集群系统资源最大化利用，那首先要看业务对系统资源的需求情况。经过对线上业务的梳理，通常可将这些业务分为如下几类：
>
> 1. 硬盘容量敏感型业务：这类业务对读写延迟以及吞吐量都没有很大的要求，唯一的需要就是硬盘容量。比如大多数离线读写分析业务，上层应用一般每隔一段时间批量写入大量数据，然后读取也是定期批量读取大量数据。特点：离线写、离线读，需求硬盘容量
>
> 2. 带宽敏感型业务：这类业务大多数写入吞吐量很大，但对读取吞吐量没有什么要求。比如日志实时存储业务，上层应用通过kafka将海量日志实时传输过来，要求能够实时写入，而读取场景一般是离线分析或者在业务遇到异常的时候对日志进行检索。特点：在线写、离线读，需求带宽
>
> 3. IO敏感型业务：相比前面两类业务来说，IO敏感型业务一般都是较为核心的业务。这类业务对读写延迟要求较高，尤其对于读取延迟通常在100ms以内，部分业务可能要求更高。比如在线消息存储系统、历史订单系统、实时推荐系统等。特点：在（离）线写、在线读，需求内存、高IO介质
>
> 4. 对于CPU资源，HBase本身就是CPU敏感型系统，主要用于数据块的压缩/解压缩，基本所有业务都对CPU有共同的需求，而单纯的HDFS存储对CPU要求不高

**一个集群想要资源利用率最大化，一个思路就是各个业务之间‘扬长避短’，合理搭配，各取所需。实际上就是上述几种类型的业务能够混合分布，建议不要将同一种类型的业务太多分布在同一个集群。因此一个集群理论上资源利用率比较高效的配置为：硬盘敏感型业务 ＋ 带宽敏感型业务 ＋ IO敏感型业务。**

> 另外，集群业务规划的时候除了考虑资源使用率最大化这个问题之外，还需要考虑实际运维的需求。建议将核心业务和非核心业务分布在同一个集群，强烈建议不要将太多核心业务同时分布在同一个集群。这主要有两方面的考虑：
>
> 1. 一方面是因为核心业务共享资源必然会产生竞争，一旦出现竞争无论哪个核心业务出现问题都不是我们愿意看到的；
>
> 2. 另一方面在特殊场景下方便运维童鞋进行降级处理，比如类似于淘宝双十一这类大促活动，某个核心业务预期会有很大的流量涌入，为了保证核心业务的平稳，在资源共享的情况下只能牺牲其他非核心业务，在和非核心业务方充分交流沟通的基础上限制这些业务的资源使用，在流量极限的时候甚至可以直接停掉这些非核心业务。试想，如果是很多核心业务共享集群的话，哪个核心业务愿意轻易让路？

## 真实集群规划

![1621138298645](./assets\1621138298645.png)



Hadoop 集群实际上就是在一组通过网络连接的物理计算机组成的集群上安装部署Hadoop 相关的软件。所以，Hadoop 集群规划包括两大模块：

- (1) 物理集群规划

- (2) 软件集群规划

根据实际业务需求，确定哪些软件运行在哪些物理机上，最终以一个整体来对外提供大数据的相关服务。

### 物理集群的规划

1. Hadoop 物理集群规划主要解决的两个问题：

   - （1）准备什么样的物理计算机？——机器选型

   - （2）准备多少台物理计算机？——集群规模

2. 机器选择

   - （1）小型机：价格在百万级别以上，成本太高，Hadoop 的初衷是致力于运行在廉价的机器之上，这违背了 Hadoop 的初衷。所以不推荐使用。

   - （2）PCServer：综合考虑价格和性能，这是第一选择。

   - （3）云主机（比如阿里云、腾讯云）：一般创业初期的互联网公司会选用，因为资金不足，且数据量是逐步增大，可以采用此方式。

   - （4）普通 PC：稳定性不好，可用于实验环境。

   - （5）虚拟机：无奈，学习使用。

**总结**：

- (1) 企业开发环境物理集群优选 PCServer

- (2) 学习实验环境物理集群优选虚拟机

### 集群的规模

1. 影响集群规模的因素:

   1. 功能性要求

      数据量包括现有的全量数据、一定时期内的增量数据、数据的副本个数、临时数据、

      日志数据、安装包等等。

   2. 每台计算机的资源的配置

   3. 非功能性的要求

      1. 可靠性要求
      2. 可用性要求
      3. 容错性要求

      比如，目前公司过去一年历史数据量为500G，为了保证可靠地容错性，至少要冗余3份，冗余后为1500G。

      除此之外，还要考虑数据的不断增长，根据历史数据推算，每年用户量增长1倍，明年一年（或未来三年，根据公司财力来进行规划）内数据将达到3000G，加上1500G历史数据，共需存储4500G。再加上日志数据和软件临时数据等，需提前准备至少5000G（5T）的磁盘容量。假设每台服务器500G的SSD容量，则需要至少10台服务器。

   **总结**

   - 根据实际业务需求的不同，企业物理集群规模大小也各不相同。几台、十几台、几十台、成百上千台的都有。

   - 目前先部署三个节点的分布式集群。

### 软件集群规划

- 软件选择

> 即选择使用什么样的软件？哪个版本？是否稳定？各个软件之间是否兼容？
>
> 项目软件选型：
>
> jdk：Jdk1.8
>
> Scala2.11.8
>
> CDH 6.2.1： zookeeper-3.4.5-cdh6.2.1、hadoop-3.0.0-cdh6.2.1，hive-2.1.1-cdh6.2.1、hue-4.3.0-cdh6.2.1
>
> sqoop-1.4.7-cdh6.2.1
>
> Mysql 5.7

### 主机规划

主机规划即哪台机器上部署哪些软件。

![1621140823342](./assets\1621140823342.png)

总结：

（1）单节点集群是把所有软件都部署在同一台机器上 

（2）分布式集群是按照主机规划把对应的软件部署在对应的机器上

# Linux虚拟机环境

![1621141389877](./assets\1621141389877.png)

## **按照Linux项目的配置方法**

- 配置静态IP
- 关闭防火墙
- 关闭selinux
- 配置主机名
- 配置主机映射
- 设置时钟同步
- 安装对应版的MySQL
- 重启

## **下载包**

下载CDH包

下载地址：	<https://archive.cloudera.com/cdh6/6.2.1/parcels/>

![1621141811491](./assets\1621141811491.png)

**只需要下载对应系统下的包即可，我们使用的是Centos7，所以只需要下载后缀为el7的三个包即可。**

下载CM包

下载地址：	<https://archive.cloudera.com/cm6/6.2.1/redhat7/yum/RPMS/x86_64/>

![1621141906794](./assets\1621141906794.png)

下载秘钥文件

下载地址：	<https://archive.cloudera.com/cm6/6.2.1/>

![1621141963099](./assets\1621141963099.png)

安装依赖

```sh
yum install -y cyrus-sasl-plain cyrus-sasl-gssapi portmap fuse-libs bind-utils libxslt fuse
yum install -y /lib/lsb/init-functions createrepo deltarpm python-deltarpm
yum install -y mod_ssl openssl-devel python-psycopg2 MySQL-python
```

以上的三台都做

### 安装httpd

只在hadoop01 上安装

```sh
yum install httpd
yum install createrepo
```

### 安装httpd服务

因为在实际的生产环境中，很有可能是不能联网的，或者从外网下载的速度较慢。这时我们通过本地的镜像来进行安装，效率会大大提升。

只在第一台服务器执行即可

```sh
#前面已执行过
yum install httpd -y

#启动httpd服务
systemctl start httpd.service

#进入域名根目录
cd /var/www/html/
#需要创建和官网路径一致的目录，yum安装时才能够正确的找到
mkdir -p cm6/6.2.1/redhat7/yum/RPMS/x86_64/
```

- 上传cm6中的文件到目录：/var/www/html/cm6/6.2.1/redhat7/yum/RPMS/x86_64/

- 上传allkeys.asc文件到目录：/var/www/html/cm6/6.2.1/

- 访问测试：<http://hadoop01/cm6/6.2.1/redhat7/yum/RPMS/x86_64/>

  ![1621144288305](./assets\1621144288305.png)

  ### **生成repodata目录**

![1621144323450](./assets\1621144323450.png)

> 可以看到在官网的地址中，有一个repodata目录，而我们新搭建的http服务中是没有的。此文件夹是不能直接复制的，需要使用脚本生成出来，是对现有文件结构的描述。如果http中的文件内容有变更，需要删除后重新生成repodata目录，以在web中生效。

~~~sql
cd /var/www/html/cm6/6.2.1/redhat7/yum
createrepo .
~~~

![1621144382431](./assets\1621144382431.png)

访问web链接，确认repodata目录已存在：

[Index of /cm6/6.2.1/redhat7/yum](http://hadoop01/cm6/6.2.1/redhat7/yum/)

![1621144841446](./assets\1621144841446.png)

### **配置本地yum源**

只在第一台服务器执行即可

~~~sql
cd /etc/yum.repos.d/
vim cloudera-manager.repo

[cloudera-manager]
name=Cloudera Manager
baseurl=http://hadoop01/cm6/6.2.1/redhat7/yum/
gpgcheck=0
enabled=1

yum clean all
yum list | grep cloudera
~~~

### 1.1  **cloudera-scm用户**

centos7要求必须有，centos6没有要求，每一台服务器都需要执行。

```sh
useradd cloudera-scm
passwd cloudera-scm
# 密码输入123456
#免密钥登录
echo "cloudera-scm ALL=(root)NOPASSWD:ALL" >> /etc/sudoers
#测试用户
su - cloudera-scm
exit
```

## Linux配置优化

### Swappiness虚拟内存

swappiness是Linux的一个内核参数，控制系统在使用swap虚拟内存时，内存使用的相对权重。

swappiness参数值可设置范围在0到100之间。 此参数值越低，就会让Linux系统尽量少用swap虚拟内存分区，多用内存；参数值越高就是反过来，使内核更多的去使用swap空间。推荐设置为10。根据服务器硬件配置会有变化。

~~~sh
临时生效：
sysctl -w vm.swappiness=10

永久生效：
echo "vm.swappiness=10" >> /etc/sysctl.conf
~~~

### 关闭内存动态分配（内存页透明化）

> 自CentOS6版本开始引入了Transparent Huge Pages(THP透明的巨大的页面)，从CentOS7版本开始，该特性默认就会启用。
>
> Transparent HugePages是在运行时动态分配内存的，而标准的HugePages是在系统启动时预先分配内存，并在系统运行时不再改变。
>
> 因为Transparent HugePages是在运行时动态分配内存的，所以会带来在运行时内存分配延误。
>
> 因此，尽管THP的本意是为提升内存的性能，不过某些数据库厂商还是建议直接关闭THP(比如说Cloudera、ORACLE、MariaDB、MongoDB等)，否则可能会导致性能出现下降。

```sh
# 临时生效：
echo never > /sys/kernel/mm/transparent_hugepage/defrag
echo never > /sys/kernel/mm/transparent_hugepage/enabled

# 永久生效：
echo "echo never > /sys/kernel/mm/transparent_hugepage/defrag" >> /etc/rc.local
echo "echo never > /sys/kernel/mm/transparent_hugepage/enabled" >> /etc/rc.local
```

### **最大文件句柄**

Maximum File Handles，大数据服务可能会打开非常大量的文件句柄。通过编辑 /etc/security/limits.conf 来增加限制，添加类似的内容：

~~~sh
* hard nofile 50000
* soft nofile 50000
~~~

第一行是指定用户，前面的 * 代表所有用户。

### **最大派生进程数**

Maximum Forked Processes，配置允许生成大量的线程。要增加Linux允许的数量，编辑 /etc/security/limits.conf

```sh
* hard  nproc  10000
* soft  nproc  10000

```

发行版Linux可能需要通过添加来编辑 /etc/security/limits.d/20-nproc.conf

```sh
* soft nproc 10000
```

### TCP Socket端口数

增加可用的TCP套接字端口数(Increase the number of TCP socket ports available)，如果你的流程会在很短的时间内创建并拆除大量socket，这一点尤为重要。

```sh
sudo sysctl -w net.ipv4.ip_local_port_range ="10000 65000"
```

### 缩减Socket闲置时间

socket连接闲置太长时间会影响并发量，设置socket在保持TIMED_WAIT状态的时间，能够快速创建和销毁新socket。

```sh
sudo sysctl -w net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait ="1"
```

### **提高IO性能**

默认的方式下linux会把文件访问的时间atime做记录，文件系统在文件被访问、创建、修改等的时候记录下了文件的一些时间戳，比如：文件创建时间、最近一次修改时间和最近一次访问时间；这在绝大部分的场合都是没有必要的。

因为系统运行的时候要访问大量文件，如果能减少一些动作（比如减少时间戳的记录次数等）将会显著提高磁盘 IO 的效率、提升文件系统的性能。

如果遇到机器IO负载高或是CPU WAIT高的情况，可以尝试使用noatime和nodiratime禁止记录最近一次访问时间戳。你会发现吞吐量有惊人的提高。

比如我要在根文件系统使用noatime，可以编辑/etc/fstab文件，如下

```
/dev/mapper/centos-root /      xfs     defaults,noatime        0 0
UUID=47f23406-2cda-4601-93b6-09030b30e2dd /boot     xfs     defaults        0 0
/dev/mapper/centos-swap swap     swap    defaults        0 0
```

修改后重新挂载

~~~sh
mount -o remount /
或者
mount -o remount /boot

~~~



echo "vm.swappiness=10" >> /etc/sysctl.conf